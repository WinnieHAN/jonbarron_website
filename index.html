<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="logo-stu.png">
  <title>Wenjuan Han</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Wenjuan Han</name>
        </p>
<!--         <p>I am a senior research scientist at <a href="https://research.google.com/">Google Research</a>, where I work on computer vision and computational photography. At Google I've worked on <a href="http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="http://googleresearch.blogspot.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://research.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>.
        </p> -->
<!--         <p>
          I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've spent time at <a href="https://en.wikipedia.org/wiki/Google_X">Google[x]</a>, <a href="http://groups.csail.mit.edu/vision/welcome/">MIT CSAIL</a>, <a href="http://www.captricity.com/">Captricity</a>, <a href="https://www.nasa.gov/ames">NASA Ames</a>, <a href="http://www.google.com/">Google NYC</a>, the <a href="http://mrl.nyu.edu/">NYU MRL</a>, <a href="http://www.nibr.com/">Novartis</a>, and <a href="http://www.astrometry.net/">Astrometry.net</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
        </p> -->
          <p>
          I am now a research fellow in National University of Singapore. I got the PHD Degree at <a href="http://www.Shanghaitech.edu.cn/">ShanghaiTech University</a>, where I was advised by <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>. I did my bachelors at the <a href="http://www.njupt.edu.cn/">Nanjing University of Posts and Telecommunications</a>.
        </p>
        <p align=center>
          <a href="hanwj@shanghaitech.edu.cn">Email</a> &nbsp/&nbsp
          <a href="hanwj_CV.pdf">CV</a> &nbsp/&nbsp
          <!-- <a href="JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
          <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp -->
          <a href="http://www.linkedin.com/in/wenjuan-han-b85b91147/"> LinkedIn </a>
        </p>
        </td>
        <td width="33%">
        <img src="pic_2.png">
        </td>
      </tr>
      </table>
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
          My current research focuses on the study of probabilistic/neural models and follows two researching paths: (1) grammar-based representation, inference, and unsupervised learning; and (2) the application of unsupervised learning approaches with hidden variables in a variety of artificial intelligence areas including grammar induction, POS induction and perceptual grouping. Representative papers are <span class="highlight">highlighted</span>.
          </p>
        </td>
      </tr>
      </table>

<!-- 3 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='2020-1.PNG'></div>
        <img src='2020-1.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
    <a href="https://www.aclweb.org/anthology/2020.acl-main.333.pdf">
            <papertitle>Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation</papertitle>
    </a>
    <br>
    <a href="https://www.aclweb.org/anthology/2020.acl-main.333.pdf">Bo Pang</a>,
    <a href="https://www.aclweb.org/anthology/2020.acl-main.333.pdf">Erik Nijkamp</a>,
    <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="https://www.aclweb.org/anthology/2020.acl-main.333.pdf">Linqi Zhou</a>,
    <a href="https://www.aclweb.org/anthology/2020.acl-main.333.pdf">Yixian Liu</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
    <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>The 58th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2020 <br>
        <p></p>
        <p>Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) $n$-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.</p>
      </td>
    </tr>
  </table>
<!-- 3 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='4-1.PNG'></div>
        <img src='4-1.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
    <a href="http://faculty.sist.shanghaitech.edu.cn/faculty/tukw/acl19disc.pdf">
            <papertitle>Enhancing Unsupervised Generative Dependency Parser with Contextual Information</papertitle>
    </a>
    <br>
    <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="https://yongjiang.ml">Yong Jiang</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
    <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>The 57th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2019 <br>
        <p></p>
        <p>Most of the unsupervised dependency parsers are based on probabilistic generative models that learn the joint distribution of the given sentence and its parse. Probabilistic generative models usually explicit decompose the desired dependency tree into factorized grammar rules, which lack the global features of the entire sentence. In this paper, we propose a novel probabilistic model called discriminative neural dependency model with valence (D-NDMV) that generates a sentence and its parse from a continuous latent representation, which encodes global contextual information of the generated sentence. We propose two approaches to model the latent representation: the first deterministically summarizes the representation from the sentence and the second probabilistically models the representation conditioned on the sentence.
Our approach can be regarded as a new type of autoencoder model to unsupervised dependency parsing that combines the benefits of both generative and discriminative techniques.
In particular, our approach breaks the context-free independence assumption in previous generative approaches and therefore becomes more expressive. 
Our extensive experimental results on seventeen datasets from various sources show that our approach achieves competitive accuracy compared with both generative and discriminative state-of-the-art unsupervised dependency parsers.</p>
      </td>
    </tr>
  </table>
<!-- 3 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='7-1.PNG'></div>
        <img src='7-1.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
    <a href="waiting">
            <papertitle>Multilingual Grammar Induction with Continuous Language Identification</papertitle>
    </a>
    <br>
    <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Ge Wang</a>,
    <a href="https://yongjiang.ml">Yong Jiang</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
    <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 2019 <br>
        <p></p>
        <p>The key to multilingual grammar induction is to couple grammar parameters of different languages together by exploiting the similarity between languages. Previous work relies on linguistic phylogenetic knowledge to specify similarity between languages. In this work, we propose a novel universal grammar induction approach that represents language identities with continuous vectors and employs a neural network to predict grammar parameters based on the representation. Without any prior linguistic phylogenetic knowledge, we automatically capture similarity between languages with the vector representations and softly tie the grammar parameters of different languages. In our experiments, we apply our approach to 15 languages across 8 language families and subfamilies in the Universal Dependency Treebank dataset, and we observe substantial performance gain on average over monolingual and multilingual baselines.</p>
      </td>
    </tr>
  </table>
  <!-- 3 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='8-1.PNG'></div>
        <img src='8-1.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
    <a href="waiting">
            <papertitle>A Regularization-based Framework for Bilingual Grammar Induction</papertitle>
    </a>
    <br>
    <a href="https://yongjiang.ml">Yong Jiang</a>,
    <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
    <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 2019 <br>
        <p></p>
        <p>Grammar induction aims to discover syntactic structures from unannotated sentences.
In this paper, we propose a framework in which the learning process of the grammar model of one language is influenced by knowledge from the model of another language. Unlike previous work on multilingual grammar induction, our approach does not rely on any external resources, such as parallel corpora, word alignments or linguistic phylogenetic trees. We propose three regularization methods that encourage similarity between model parameters, dependency edge scores, and parse trees respectively. We deploy our methods on a state-of-the-art unsupervised discriminative parser and evaluate it on both transfer grammar induction and bilingual grammar induction. Empirical results on multiple languages show that our methods outperform strong baselines.</p>
      </td>
    </tr>
  </table>
  <!-- 3 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='5-1.PNG'></div>
        <img src='5-1.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
    <a href="https://www.sciencedirect.com/science/article/pii/S0925231219305284">
            <papertitle>Lexicalized Neural Unsupervised Dependency Parsing</papertitle>
    </a>
    <br>
    <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="https://yongjiang.ml">Yong Jiang</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
    <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>Neurocomputing</em>, 2019 <br>
        <p></p>
        <p>This paper describes a neural network (NN) based probabilistic model, Neural Dependency Model with Valence (NDMV), that combines the dependency parsing with the rich nonlinear featurization of NN approaches. NDMV follows the traditional DMV model and computes the probability of a grammar rule via a feed-forward NN.
A strength of this proposed approach is the ability to learn the underlying features of input tokens. In our experiments, this capability leads to gains in both NDMV and its extension version, Lexicalized Neural Dependency Model with Valence (L-NDMV): NDMV achieves better performance on WSJ and datasets of eight additional languages in comparison with previous approaches in the basic setting; L-NDMV achieves a result that is competitive with the current state-of-the-art.</p>
      </td>
    </tr>
  </table>

<!-- 3 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='6-1.PNG'></div>
        <img src='6-1.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
    <a href="https://ieeexplore.ieee.org/abstract/document/8689031">
            <papertitle>Latent Variable Autoencoder</papertitle>
    </a>
    <br>
    <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="https://yongjiang.ml">Yong Jiang</a>,
    <a href="https://sist.shanghaitech.edu.cn/faculty/tukw/">Ge Wang</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
    <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>IEEE Access</em>, 2019 <br>
        <p></p>
        <p>Learning to discover hidden variables from unlabeled data is an important task. Traditional generative methods model the generation process of the observed variables as well as the hidden variables. However, tractable inference and learning on these models requires strong conditional independence assumptions being made among observed and hidden variables.
To tackle this limitation, we propose an autoencoder framework. The encoder produces an intermediate representation from the observed variables and the decoder is a generative latent variable model conditioned on the intermediate representation that tries to generate the hidden variables as well as reconstruct the observed variables.
We introduce three variant models of our framework with either a deterministic or a stochastic encoding process.
To optimize our model, we propose an algorithm similar to the classic EM algorithm that supports online learning for large-scale datasets.
The flexibility of our framework allows us to apply it to various scenarios where the explicit inference of hidden variables is desired. 
We discuss applications of our framework to the perceptual grouping task and the POS induction task. Our experiments on the two tasks demonstrate that our framework can achieve better performance than vanilla latent variable generative models.</p>
      </td>
    </tr>
  </table>
<!-- 3 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='3-1.PNG'></div>
        <img src='3-1.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
	  <a href="https://arxiv.org/abs/1708.00801">
            <papertitle>Dependency Grammar Induction with Neural Lexicalization and Big Training Data</papertitle>
	  </a>
	  <br>
	  <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="https://yongjiang.ml">Yong Jiang</a>,
	  <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
	  <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2017 <br>
        <p></p>
        <p>We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training cor-pus size) on dependency grammar induc-tion. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Va-lence (Jiang et al., 2016). We find that L-DMV only benefits from very small de-grees of lexicalization and moderate sizes of training corpora. L-NDMV can bene-fit from big training data and lexicaliza-tion of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.</p>
      </td>
    </tr>
  </table>
<!-- 2 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='2-1.PNG'></div>
        <img src='2-2.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
    <a href="https://arxiv.org/abs/1708.00790">
            <papertitle>Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition</papertitle>
    </a>
    <br>
    <a href="https://yongjiang.ml">Yong Jiang</a>,
    <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
    <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2017 <br>
        <p></p>
        <p>Unsupervised dependency parsing aims to learn a dependency parser from unanno-tated sentences. Existing work focuses on either learning generative models us-ing the expectation-maximization algo-rithm and its variants, or learning dis-criminative models using the discrimina-tive clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decom-position method. Our method is simple and general, yet effective to capture the ad-vantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-of-the-art performance on thirty languages.</p>
      </td>
    </tr>
  </table>

<!-- 1 -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >

      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='1-2.PNG'></div>
        <img src='1-1.PNG'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>

      <td valign="top" width="75%">
    <a href="https://www.aclweb.org/anthology/D16-1073">
            <papertitle>Unsupervised Neural Dependency Parsing</papertitle>
    </a>
    <br>
    <a href="https://yongjiang.ml">Yong Jiang</a>,
    <a href="http://hanwenjuan.com/">Wenjuan Han</a>,
    <a href="http://sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
    <!-- <strong>Jonathan T. Barron</strong> <br> -->
        <em>Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2016 <br>
        <p></p>
        <p>Unsupervised dependency parsing aims to learn a dependency grammar from text anno-tated with only POS tags. Various features and inductive biases are often used to incorpo-rate prior knowledge into learning. One use-ful type of prior information is that there exist correlations between the parameters of gram-mar rules involving different POS tags. Pre-vious work employed manually designed fea-tures or special prior distributions to encode such information. In this paper, we propose a novel approach to unsupervised dependen-cy parsing that uses a neural model to predict grammar rule probabilities based on distribut-ed representation of POS tags. The distributed representation is automatically learned from data and captures the correlations between POS tags. Our experiments show that our approach outperforms previous approaches u-tilizing POS correlations and is competitive with recent state-of-the-art approaches on nine different languages.</p>
      </td>
    </tr>
  </table>
<!--       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Course Projects</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="25%"><img src="prl.jpg" alt="prl" width="160" height="160"></td>
        <td width="75%" valign="top">
        <p>
          <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
          <papertitle>Parallelizing Reinforcement Learning</papertitle>
          </a>
          <br>
          <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>, 2009
        <p><br>
          Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.
        </p>
        </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Teaching</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="25%"><img src="pacman.jpg" alt="pacman" width="160" height="160"></td>
        <td width="75%" valign="center">
        <p>
          <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">
          <papertitle>CS188 - Fall 2010 (GSI)</papertitle>
          </a>
          <br><br>
          <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">
          <papertitle>CS188 - Spring 2011 (GSI)</papertitle>
          </a>
          <br>
        </p>
        </td>
      </tr>
      </table> -->


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
           <a href="https://github.com/jonbarron/jonbarron_website"><strong>Website template credits</strong></a>
	    </font>
        </p>
        </td>
      </tr>
      </table>

      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
    </td>
    </tr>
  </table>
  </body>
</html>
